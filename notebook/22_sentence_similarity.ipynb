{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# 한국어에 특화된 Sentence-BERT 모델 로드\n",
    "# 'snunlp/KR-SBERT-V40K-klueNLI-58B' 또는 'jhgan/ko-sroberta-multitask' 등\n",
    "# 다양한 한국어 SBERT 모델이 Hugging Face Hub에 있습니다.\n",
    "# 여기서는 'jhgan/ko-sroberta-multitask'를 예시로 사용합니다.\n",
    "model_name = 'jhgan/ko-sroberta-multitask'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"'{model_name}' 모델 로드 완료.\")\n",
    "\n",
    "# 비교할 문장들\n",
    "sentences = [\n",
    "    \"오늘 날씨가 정말 좋네요.\",\n",
    "    \"오늘은 햇살이 따뜻하고 맑습니다.\",\n",
    "    \"점심 메뉴는 무엇인가요?\",\n",
    "    \"지금 비가 오고 있어요.\",\n",
    "    \"저는 IT 엔지니어입니다.\"\n",
    "]\n",
    "\n",
    "# 문장 임베딩 생성\n",
    "sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "# 문장 간 코사인 유사도 계산\n",
    "# 예시 1: 의미가 유사한 두 문장\n",
    "similarity_1_2 = util.cos_sim(sentence_embeddings[0], sentence_embeddings[1])\n",
    "print(f\"'{sentences[0]}'와 '{sentences[1]}'의 유사도: {similarity_1_2.item():.4f}\")\n",
    "\n",
    "# 예시 2: 의미가 다른 두 문장\n",
    "similarity_1_3 = util.cos_sim(sentence_embeddings[0], sentence_embeddings[2])\n",
    "print(f\"'{sentences[0]}'와 '{sentences[2]}'의 유사도: {similarity_1_3.item():.4f}\")\n",
    "\n",
    "# 예시 3: 의미가 완전히 다른 두 문장\n",
    "similarity_1_5 = util.cos_sim(sentence_embeddings[0], sentence_embeddings[4])\n",
    "print(f\"'{sentences[0]}'와 '{sentences[4]}'의 유사도: {similarity_1_5.item():.4f}\")\n",
    "\n",
    "# 모든 문장 쌍 간의 유사도 행렬 계산\n",
    "cosine_scores = util.cos_sim(sentence_embeddings, sentence_embeddings)\n",
    "print(\"\\n모든 문장 쌍 간의 유사도 행렬:\")\n",
    "print(cosine_scores)\n",
    "\n",
    "# 특정 문장과 가장 유사한 문장 찾기\n",
    "query_sentence = \"오늘 날씨가 맑고 좋네요.\"\n",
    "query_embedding = model.encode(query_sentence, convert_to_tensor=True)\n",
    "\n",
    "# 기존 문장들과의 유사도 계산\n",
    "similarities_to_query = util.cos_sim(query_embedding, sentence_embeddings)[0]\n",
    "\n",
    "# 유사도 순으로 정렬하여 출력\n",
    "for i, score in enumerate(similarities_to_query):\n",
    "    print(f\"'{query_sentence}'와 '{sentences[i]}'의 유사도: {score.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embedding-ota6bejb-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
